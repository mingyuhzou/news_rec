{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2026-02-14T03:05:01.534796287Z",
     "start_time": "2026-02-14T03:04:59.511970362Z"
    }
   },
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from config.common import cfg\n",
    "import os\n",
    "from os.path import join\n",
    "import polars as pl\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ming/miniconda3/envs/llm/lib/python3.12/site-packages/torch/cuda/__init__.py:174: UserWarning: CUDA initialization: Unexpected error from cudaGetDeviceCount(). Did you run some cuda functions before calling NumCudaDevices() that might have already set an error? Error 804: forward compatibility was attempted on non supported HW (Triggered internally at /pytorch/c10/cuda/CUDAFunctions.cpp:109.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-14T03:06:08.680792846Z",
     "start_time": "2026-02-14T03:06:08.660275070Z"
    }
   },
   "cell_type": "code",
   "source": "news = pl.read_parquet(join(cfg['data_path'], 'news.parquet'))",
   "id": "397c2d4d7a449155",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-14T03:06:10.035399875Z",
     "start_time": "2026-02-14T03:06:09.830050402Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train=pl.read_parquet(join(cfg['train_data_path'], 'behaviors.parquet'))\n",
    "dev=pl.read_parquet(join(cfg['dev_data_path'], 'behaviors.parquet'))"
   ],
   "id": "32523a227141c284",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-14T03:06:11.266315645Z",
     "start_time": "2026-02-14T03:06:11.257215297Z"
    }
   },
   "cell_type": "code",
   "source": "news.columns",
   "id": "947d317a606518b7",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['news_id',\n",
       " 'category',\n",
       " 'subcategory',\n",
       " 'title',\n",
       " 'abstract',\n",
       " 'url',\n",
       " 'title_entities',\n",
       " 'abstract_entities']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-14T03:06:13.747360178Z",
     "start_time": "2026-02-14T03:06:12.346784702Z"
    }
   },
   "cell_type": "code",
   "source": [
    "userID_mapping = {}\n",
    "itemID_mapping = {}\n",
    "\n",
    "for line in train.iter_rows(named=True):\n",
    "    userID = line['user_id']\n",
    "    history = line['history']\n",
    "    for itemID in history:\n",
    "        if userID not in userID_mapping:\n",
    "            userID_mapping[userID] = len(userID_mapping) + 1\n",
    "\n",
    "        if itemID not in itemID_mapping:\n",
    "            itemID_mapping[itemID] = len(itemID_mapping) + 1\n",
    "for line in dev.iter_rows(named=True):\n",
    "    userID = line['user_id']\n",
    "    history = line['history']\n",
    "    for itemID in history:\n",
    "        if userID not in userID_mapping:\n",
    "            userID_mapping[userID] = len(userID_mapping) + 1\n",
    "\n",
    "        if itemID not in itemID_mapping:\n",
    "            itemID_mapping[itemID] = len(itemID_mapping) + 1\n",
    "for itemID in news.iter_rows(named=True):\n",
    "    itemID=itemID['news_id']\n",
    "    if itemID not in itemID_mapping:\n",
    "            itemID_mapping[itemID] = len(itemID_mapping) + 1\n",
    "\n",
    "np.save(cfg['user_dict'], userID_mapping)\n",
    "print(\"user_num:\", len(userID_mapping))\n",
    "print(\"the first five userID mapping:\", list(userID_mapping.items())[:5])\n",
    "np.save(cfg['item_dict'], itemID_mapping)\n",
    "print(\"item_num:\", len(itemID_mapping))\n",
    "print(\"the first five itemID mapping:\", list(itemID_mapping.items())[:5])\n"
   ],
   "id": "e34f9bb6a1c2ffce",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_num: 94057\n",
      "the first five userID mapping: [('U13740', 1), ('U91836', 2), ('U73700', 3), ('U34670', 4), ('U8125', 5)]\n",
      "item_num: 65239\n",
      "the first five itemID mapping: [('N55189', 1), ('N42782', 2), ('N34694', 3), ('N45794', 4), ('N18445', 5)]\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-14T03:26:13.660250460Z",
     "start_time": "2026-02-14T03:26:13.350950375Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data = pl.read_parquet(os.path.join(cfg['dev_data_path'], \"behaviors_1.parquet\"))\n",
    "\n",
    "data_sorted = data.sort([\"user_id\", \"time\"])\n",
    "\n",
    "# 对每个用户取最后一条记录\n",
    "data = data_sorted.group_by(\"user_id\").tail(1)\n",
    "\n",
    "data.shape"
   ],
   "id": "9afa36c286ac2992",
   "outputs": [
    {
     "ename": "ColumnNotFoundError",
     "evalue": "unable to find column \"time\"; valid columns: [\"user_id\", \"history\", \"target\"]",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mColumnNotFoundError\u001B[39m                       Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[12]\u001B[39m\u001B[32m, line 3\u001B[39m\n\u001B[32m      1\u001B[39m data = pl.read_parquet(os.path.join(cfg[\u001B[33m'\u001B[39m\u001B[33mdev_data_path\u001B[39m\u001B[33m'\u001B[39m], \u001B[33m\"\u001B[39m\u001B[33mbehaviors_1.parquet\u001B[39m\u001B[33m\"\u001B[39m))\n\u001B[32m----> \u001B[39m\u001B[32m3\u001B[39m data_sorted = \u001B[43mdata\u001B[49m\u001B[43m.\u001B[49m\u001B[43msort\u001B[49m\u001B[43m(\u001B[49m\u001B[43m[\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43muser_id\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mtime\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m      5\u001B[39m \u001B[38;5;66;03m# 对每个用户取最后一条记录\u001B[39;00m\n\u001B[32m      6\u001B[39m data = data_sorted.group_by(\u001B[33m\"\u001B[39m\u001B[33muser_id\u001B[39m\u001B[33m\"\u001B[39m).tail(\u001B[32m1\u001B[39m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/llm/lib/python3.12/site-packages/polars/dataframe/frame.py:6103\u001B[39m, in \u001B[36mDataFrame.sort\u001B[39m\u001B[34m(self, by, descending, nulls_last, multithreaded, maintain_order, *more_by)\u001B[39m\n\u001B[32m   6005\u001B[39m \u001B[38;5;250m\u001B[39m\u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m   6006\u001B[39m \u001B[33;03mSort the dataframe by the given columns.\u001B[39;00m\n\u001B[32m   6007\u001B[39m \n\u001B[32m   (...)\u001B[39m\u001B[32m   6089\u001B[39m \u001B[33;03m└──────┴─────┴─────┘\u001B[39;00m\n\u001B[32m   6090\u001B[39m \u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m   6091\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mpolars\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mlazyframe\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m QueryOptFlags\n\u001B[32m   6093\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m (\n\u001B[32m   6094\u001B[39m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mlazy\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   6095\u001B[39m \u001B[43m    \u001B[49m\u001B[43m.\u001B[49m\u001B[43msort\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   6096\u001B[39m \u001B[43m        \u001B[49m\u001B[43mby\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   6097\u001B[39m \u001B[43m        \u001B[49m\u001B[43m*\u001B[49m\u001B[43mmore_by\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   6098\u001B[39m \u001B[43m        \u001B[49m\u001B[43mdescending\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdescending\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   6099\u001B[39m \u001B[43m        \u001B[49m\u001B[43mnulls_last\u001B[49m\u001B[43m=\u001B[49m\u001B[43mnulls_last\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   6100\u001B[39m \u001B[43m        \u001B[49m\u001B[43mmultithreaded\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmultithreaded\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   6101\u001B[39m \u001B[43m        \u001B[49m\u001B[43mmaintain_order\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmaintain_order\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   6102\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m-> \u001B[39m\u001B[32m6103\u001B[39m \u001B[43m    \u001B[49m\u001B[43m.\u001B[49m\u001B[43mcollect\u001B[49m\u001B[43m(\u001B[49m\u001B[43moptimizations\u001B[49m\u001B[43m=\u001B[49m\u001B[43mQueryOptFlags\u001B[49m\u001B[43m.\u001B[49m\u001B[43m_eager\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   6104\u001B[39m )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/llm/lib/python3.12/site-packages/polars/_utils/deprecation.py:97\u001B[39m, in \u001B[36mdeprecate_streaming_parameter.<locals>.decorate.<locals>.wrapper\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m     93\u001B[39m         kwargs[\u001B[33m\"\u001B[39m\u001B[33mengine\u001B[39m\u001B[33m\"\u001B[39m] = \u001B[33m\"\u001B[39m\u001B[33min-memory\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m     95\u001B[39m     \u001B[38;5;28;01mdel\u001B[39;00m kwargs[\u001B[33m\"\u001B[39m\u001B[33mstreaming\u001B[39m\u001B[33m\"\u001B[39m]\n\u001B[32m---> \u001B[39m\u001B[32m97\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunction\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/llm/lib/python3.12/site-packages/polars/lazyframe/opt_flags.py:326\u001B[39m, in \u001B[36mforward_old_opt_flags.<locals>.decorate.<locals>.wrapper\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    323\u001B[39m         optflags = cb(optflags, kwargs.pop(key))  \u001B[38;5;66;03m# type: ignore[no-untyped-call,unused-ignore]\u001B[39;00m\n\u001B[32m    325\u001B[39m kwargs[\u001B[33m\"\u001B[39m\u001B[33moptimizations\u001B[39m\u001B[33m\"\u001B[39m] = optflags\n\u001B[32m--> \u001B[39m\u001B[32m326\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunction\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/llm/lib/python3.12/site-packages/polars/lazyframe/frame.py:2440\u001B[39m, in \u001B[36mLazyFrame.collect\u001B[39m\u001B[34m(self, type_coercion, predicate_pushdown, projection_pushdown, simplify_expression, slice_pushdown, comm_subplan_elim, comm_subexpr_elim, cluster_with_columns, collapse_joins, no_optimization, engine, background, optimizations, **_kwargs)\u001B[39m\n\u001B[32m   2438\u001B[39m \u001B[38;5;66;03m# Only for testing purposes\u001B[39;00m\n\u001B[32m   2439\u001B[39m callback = _kwargs.get(\u001B[33m\"\u001B[39m\u001B[33mpost_opt_callback\u001B[39m\u001B[33m\"\u001B[39m, callback)\n\u001B[32m-> \u001B[39m\u001B[32m2440\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m wrap_df(\u001B[43mldf\u001B[49m\u001B[43m.\u001B[49m\u001B[43mcollect\u001B[49m\u001B[43m(\u001B[49m\u001B[43mengine\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcallback\u001B[49m\u001B[43m)\u001B[49m)\n",
      "\u001B[31mColumnNotFoundError\u001B[39m: unable to find column \"time\"; valid columns: [\"user_id\", \"history\", \"target\"]"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-14T03:26:42.733291286Z",
     "start_time": "2026-02-14T03:26:42.724613363Z"
    }
   },
   "cell_type": "code",
   "source": "data.head()",
   "id": "eddfb5c8fa568fc7",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "shape: (5, 3)\n",
       "┌─────────┬───────────────────────┬───────────┐\n",
       "│ user_id ┆ history               ┆ target    │\n",
       "│ ---     ┆ ---                   ┆ ---       │\n",
       "│ str     ┆ list[i64]             ┆ list[i64] │\n",
       "╞═════════╪═══════════════════════╪═══════════╡\n",
       "│ U78223  ┆ [4570, 350, … 16034]  ┆ [16034]   │\n",
       "│ U5885   ┆ [1965, 5607, … 58593] ┆ [58593]   │\n",
       "│ U78886  ┆ [1824, 1187, … 49992] ┆ [49992]   │\n",
       "│ U35301  ┆ [1065, 4512, … 60644] ┆ [60644]   │\n",
       "│ U32668  ┆ [5809, 3580, … 48776] ┆ [48776]   │\n",
       "└─────────┴───────────────────────┴───────────┘"
      ],
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 3)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>user_id</th><th>history</th><th>target</th></tr><tr><td>str</td><td>list[i64]</td><td>list[i64]</td></tr></thead><tbody><tr><td>&quot;U78223&quot;</td><td>[4570, 350, … 16034]</td><td>[16034]</td></tr><tr><td>&quot;U5885&quot;</td><td>[1965, 5607, … 58593]</td><td>[58593]</td></tr><tr><td>&quot;U78886&quot;</td><td>[1824, 1187, … 49992]</td><td>[49992]</td></tr><tr><td>&quot;U35301&quot;</td><td>[1065, 4512, … 60644]</td><td>[60644]</td></tr><tr><td>&quot;U32668&quot;</td><td>[5809, 3580, … 48776]</td><td>[48776]</td></tr></tbody></table></div>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-14T03:26:54.001813721Z",
     "start_time": "2026-02-14T03:26:53.964210413Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def prepare_data(data_dict):\n",
    "    \"数据保存polar形式\"\n",
    "    rows = []\n",
    "    for userID, item_sequenec in data_dict.items():\n",
    "        history = item_sequenec[:-1]\n",
    "        target = item_sequenec[-1]\n",
    "        rows.append({'user_id': userID, 'history': history, 'target': target})\n",
    "    return pl.DataFrame(rows)\n",
    "\n",
    "\n",
    "\"划分训练集，测试集，验证集\"\n",
    "train_data = {}\n",
    "val_data = {}\n",
    "test_data = {}\n",
    "\n",
    "for row in data.iter_rows(named=True):\n",
    "    userID = row[\"user_id\"]\n",
    "    item_sequence = [itemID_mapping[t] for t in  row[\"history\"]]\n",
    "    if len(item_sequence) > 2:\n",
    "        train_data[userID] = item_sequence[:-2]\n",
    "        val_data[userID] = item_sequence[:-1]\n",
    "        test_data[userID] = item_sequence\n",
    "\n",
    "print(\"training data:\", list(train_data.items())[:5])\n",
    "print(\"validation data:\", list(val_data.items())[:5])\n",
    "print(\"testing data:\", list(test_data.items())[:5])\n",
    "\n",
    "train_df = prepare_data(train_data)\n",
    "print(\"\\nTraining data shape:\", train_df.shape)\n",
    "print(\"the first 3 rows of training data:\\n\", train_df.head(3))\n",
    "test_df = prepare_data(test_data)\n",
    "print(\"\\nTesting data shape:\", test_df.shape)\n",
    "print(\"the first 3 rows of testing data:\\n\", test_df.head(3))\n",
    "val_df = prepare_data(val_data)\n",
    "print(\"\\nValidation data shape:\", val_df.shape)\n",
    "print(\"the first 3 rows of validation data:\\n\", val_df.head(3))\n",
    "\n",
    "test_df.shape"
   ],
   "id": "3cdb7dc827c97e06",
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "4570",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyError\u001B[39m                                  Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[15]\u001B[39m\u001B[32m, line 18\u001B[39m\n\u001B[32m     16\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m row \u001B[38;5;129;01min\u001B[39;00m data.iter_rows(named=\u001B[38;5;28;01mTrue\u001B[39;00m):\n\u001B[32m     17\u001B[39m     userID = row[\u001B[33m\"\u001B[39m\u001B[33muser_id\u001B[39m\u001B[33m\"\u001B[39m]\n\u001B[32m---> \u001B[39m\u001B[32m18\u001B[39m     item_sequence = [\u001B[43mitemID_mapping\u001B[49m\u001B[43m[\u001B[49m\u001B[43mt\u001B[49m\u001B[43m]\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m t \u001B[38;5;129;01min\u001B[39;00m  row[\u001B[33m\"\u001B[39m\u001B[33mhistory\u001B[39m\u001B[33m\"\u001B[39m]]\n\u001B[32m     19\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(item_sequence) > \u001B[32m2\u001B[39m:\n\u001B[32m     20\u001B[39m         train_data[userID] = item_sequence[:-\u001B[32m2\u001B[39m]\n",
      "\u001B[31mKeyError\u001B[39m: 4570"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-14T03:08:09.435277731Z",
     "start_time": "2026-02-14T03:08:09.316039619Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"Data saved to parquet files.\")\n",
    "train_df.write_parquet(os.path.join(cfg['dev_data_path'], \"train_df.parquet\"))\n",
    "test_df.write_parquet(os.path.join(cfg['dev_data_path'], \"test_df.parquet\"))\n",
    "val_df.write_parquet(os.path.join(cfg['dev_data_path'], \"valid_df.parquet\"))"
   ],
   "id": "996961938f5e8eef",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to parquet files.\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-14T02:29:18.052051489Z",
     "start_time": "2026-02-14T02:29:17.242382607Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_path = cfg['emb_model_path']\n",
    "\n",
    "# 加载本地模型\n",
    "model = SentenceTransformer(model_path, device='cuda')"
   ],
   "id": "b355255a4e4a3a61",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Loading weights:   0%|          | 0/99 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f684ed665ea44aa682119732e8c842ee"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-14T02:29:21.745580672Z",
     "start_time": "2026-02-14T02:29:18.063234544Z"
    }
   },
   "cell_type": "code",
   "source": [
    "item_embeddings = []\n",
    "for row in tqdm(news.iter_rows(named=True),total=news.shape[0]):\n",
    "    itemID = itemID_mapping[row['news_id']]\n",
    "\n",
    "    semantics = f\"Title: {row.get('title','')}\\n\"\n",
    "\n",
    "    embedding = model.encode(semantics)\n",
    "    item_embeddings.append({'ItemID': itemID, 'embedding': embedding.tolist()})\n",
    "\n",
    "# Convert to DataFrame\n",
    "item_emb_df = pd.DataFrame(item_embeddings)\n",
    "\n",
    "print(\"\\nItem embeddings DataFrame shape:\", item_emb_df.shape)\n",
    "print(\"The first 3 rows of item embeddings DataFrame:\\n\", item_emb_df.head(3))\n",
    "\n",
    "# Save to parquet file\n",
    "item_emb_df.to_parquet(join(cfg['embed_path'],'item_emb_title.parquet'), index=False)"
   ],
   "id": "82df19befc09759b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/65238 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c685f20946484f81bf3d7f07ae5c7cbe"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[8]\u001B[39m\u001B[32m, line 7\u001B[39m\n\u001B[32m      3\u001B[39m     itemID = itemID_mapping[row[\u001B[33m'\u001B[39m\u001B[33mnews_id\u001B[39m\u001B[33m'\u001B[39m]]\n\u001B[32m      5\u001B[39m     semantics = \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mTitle: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mrow.get(\u001B[33m'\u001B[39m\u001B[33mtitle\u001B[39m\u001B[33m'\u001B[39m,\u001B[33m'\u001B[39m\u001B[33m'\u001B[39m)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m7\u001B[39m     embedding = \u001B[43mmodel\u001B[49m\u001B[43m.\u001B[49m\u001B[43mencode\u001B[49m\u001B[43m(\u001B[49m\u001B[43msemantics\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m      8\u001B[39m     item_embeddings.append({\u001B[33m'\u001B[39m\u001B[33mItemID\u001B[39m\u001B[33m'\u001B[39m: itemID, \u001B[33m'\u001B[39m\u001B[33membedding\u001B[39m\u001B[33m'\u001B[39m: embedding.tolist()})\n\u001B[32m     10\u001B[39m \u001B[38;5;66;03m# Convert to DataFrame\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/llm/lib/python3.12/site-packages/torch/utils/_contextlib.py:116\u001B[39m, in \u001B[36mcontext_decorator.<locals>.decorate_context\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    113\u001B[39m \u001B[38;5;129m@functools\u001B[39m.wraps(func)\n\u001B[32m    114\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mdecorate_context\u001B[39m(*args, **kwargs):\n\u001B[32m    115\u001B[39m     \u001B[38;5;28;01mwith\u001B[39;00m ctx_factory():\n\u001B[32m--> \u001B[39m\u001B[32m116\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/llm/lib/python3.12/site-packages/sentence_transformers/SentenceTransformer.py:1094\u001B[39m, in \u001B[36mSentenceTransformer.encode\u001B[39m\u001B[34m(self, sentences, prompt_name, prompt, batch_size, show_progress_bar, output_value, precision, convert_to_numpy, convert_to_tensor, device, normalize_embeddings, truncate_dim, pool, chunk_size, **kwargs)\u001B[39m\n\u001B[32m   1091\u001B[39m features.update(extra_features)\n\u001B[32m   1093\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m torch.no_grad():\n\u001B[32m-> \u001B[39m\u001B[32m1094\u001B[39m     out_features = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mforward\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfeatures\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1095\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.device.type == \u001B[33m\"\u001B[39m\u001B[33mhpu\u001B[39m\u001B[33m\"\u001B[39m:\n\u001B[32m   1096\u001B[39m         out_features = copy.deepcopy(out_features)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/llm/lib/python3.12/site-packages/sentence_transformers/SentenceTransformer.py:1175\u001B[39m, in \u001B[36mSentenceTransformer.forward\u001B[39m\u001B[34m(self, input, **kwargs)\u001B[39m\n\u001B[32m   1169\u001B[39m             module_kwarg_keys = \u001B[38;5;28mself\u001B[39m.module_kwargs.get(module_name, [])\n\u001B[32m   1170\u001B[39m         module_kwargs = {\n\u001B[32m   1171\u001B[39m             key: value\n\u001B[32m   1172\u001B[39m             \u001B[38;5;28;01mfor\u001B[39;00m key, value \u001B[38;5;129;01min\u001B[39;00m kwargs.items()\n\u001B[32m   1173\u001B[39m             \u001B[38;5;28;01mif\u001B[39;00m key \u001B[38;5;129;01min\u001B[39;00m module_kwarg_keys \u001B[38;5;129;01mor\u001B[39;00m (\u001B[38;5;28mhasattr\u001B[39m(module, \u001B[33m\"\u001B[39m\u001B[33mforward_kwargs\u001B[39m\u001B[33m\"\u001B[39m) \u001B[38;5;129;01mand\u001B[39;00m key \u001B[38;5;129;01min\u001B[39;00m module.forward_kwargs)\n\u001B[32m   1174\u001B[39m         }\n\u001B[32m-> \u001B[39m\u001B[32m1175\u001B[39m     \u001B[38;5;28minput\u001B[39m = \u001B[43mmodule\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mmodule_kwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1176\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28minput\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/llm/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1749\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1750\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1751\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/llm/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1757\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1758\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1759\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1760\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1761\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1762\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1764\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1765\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/llm/lib/python3.12/site-packages/sentence_transformers/models/Transformer.py:262\u001B[39m, in \u001B[36mTransformer.forward\u001B[39m\u001B[34m(self, features, **kwargs)\u001B[39m\n\u001B[32m    239\u001B[39m \u001B[38;5;250m\u001B[39m\u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m    240\u001B[39m \u001B[33;03mForward pass through the transformer model.\u001B[39;00m\n\u001B[32m    241\u001B[39m \n\u001B[32m   (...)\u001B[39m\u001B[32m    258\u001B[39m \u001B[33;03m        - 'all_layer_embeddings': If the model outputs hidden states, contains embeddings from all layers\u001B[39;00m\n\u001B[32m    259\u001B[39m \u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m    260\u001B[39m trans_features = {key: value \u001B[38;5;28;01mfor\u001B[39;00m key, value \u001B[38;5;129;01min\u001B[39;00m features.items() \u001B[38;5;28;01mif\u001B[39;00m key \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m.model_forward_params}\n\u001B[32m--> \u001B[39m\u001B[32m262\u001B[39m outputs = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mauto_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mtrans_features\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[32m    263\u001B[39m token_embeddings = outputs[\u001B[32m0\u001B[39m]\n\u001B[32m    264\u001B[39m features[\u001B[33m\"\u001B[39m\u001B[33mtoken_embeddings\u001B[39m\u001B[33m\"\u001B[39m] = token_embeddings\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/llm/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1749\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1750\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1751\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/llm/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1757\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1758\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1759\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1760\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1761\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1762\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1764\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1765\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/llm/lib/python3.12/site-packages/transformers/models/t5/modeling_t5.py:1231\u001B[39m, in \u001B[36mT5EncoderModel.forward\u001B[39m\u001B[34m(self, input_ids, attention_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict, **kwargs)\u001B[39m\n\u001B[32m   1206\u001B[39m \u001B[38;5;250m\u001B[39m\u001B[33mr\u001B[39m\u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m   1207\u001B[39m \u001B[33;03minput_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\u001B[39;00m\n\u001B[32m   1208\u001B[39m \u001B[33;03m    Indices of input sequence tokens in the vocabulary. T5 is a model with relative position embeddings so you\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m   1227\u001B[39m \u001B[33;03m>>> last_hidden_states = outputs.last_hidden_state\u001B[39;00m\n\u001B[32m   1228\u001B[39m \u001B[33;03m```\"\"\"\u001B[39;00m\n\u001B[32m   1229\u001B[39m return_dict = return_dict \u001B[38;5;28;01mif\u001B[39;00m return_dict \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m.config.use_return_dict\n\u001B[32m-> \u001B[39m\u001B[32m1231\u001B[39m encoder_outputs = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mencoder\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1232\u001B[39m \u001B[43m    \u001B[49m\u001B[43minput_ids\u001B[49m\u001B[43m=\u001B[49m\u001B[43minput_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1233\u001B[39m \u001B[43m    \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m=\u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1234\u001B[39m \u001B[43m    \u001B[49m\u001B[43minputs_embeds\u001B[49m\u001B[43m=\u001B[49m\u001B[43minputs_embeds\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1235\u001B[39m \u001B[43m    \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[43m=\u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1236\u001B[39m \u001B[43m    \u001B[49m\u001B[43moutput_hidden_states\u001B[49m\u001B[43m=\u001B[49m\u001B[43moutput_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1237\u001B[39m \u001B[43m    \u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[43m=\u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1238\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1240\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m encoder_outputs\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/llm/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1749\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1750\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1751\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/llm/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1757\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1758\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1759\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1760\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1761\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1762\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1764\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1765\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/llm/lib/python3.12/site-packages/transformers/models/t5/modeling_t5.py:767\u001B[39m, in \u001B[36mT5Stack.forward\u001B[39m\u001B[34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, inputs_embeds, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, **kwargs)\u001B[39m\n\u001B[32m    764\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m output_hidden_states:\n\u001B[32m    765\u001B[39m     all_hidden_states = all_hidden_states + (hidden_states,)\n\u001B[32m--> \u001B[39m\u001B[32m767\u001B[39m layer_outputs = \u001B[43mlayer_module\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    768\u001B[39m \u001B[43m    \u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    769\u001B[39m \u001B[43m    \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    770\u001B[39m \u001B[43m    \u001B[49m\u001B[43mposition_bias\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    771\u001B[39m \u001B[43m    \u001B[49m\u001B[43mencoder_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    772\u001B[39m \u001B[43m    \u001B[49m\u001B[43mencoder_extended_attention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    773\u001B[39m \u001B[43m    \u001B[49m\u001B[43mencoder_decoder_position_bias\u001B[49m\u001B[43m,\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# as a positional argument for gradient checkpointing\u001B[39;49;00m\n\u001B[32m    774\u001B[39m \u001B[43m    \u001B[49m\u001B[43mpast_key_values\u001B[49m\u001B[43m=\u001B[49m\u001B[43mpast_key_values\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    775\u001B[39m \u001B[43m    \u001B[49m\u001B[43muse_cache\u001B[49m\u001B[43m=\u001B[49m\u001B[43muse_cache\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    776\u001B[39m \u001B[43m    \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[43m=\u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    777\u001B[39m \u001B[43m    \u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[43m=\u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    778\u001B[39m \u001B[43m    \u001B[49m\u001B[43mcache_position\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcache_position\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    779\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    781\u001B[39m hidden_states = layer_outputs[\u001B[32m0\u001B[39m]\n\u001B[32m    783\u001B[39m \u001B[38;5;66;03m# We share the position biases between the layers - the first layer store them\u001B[39;00m\n\u001B[32m    784\u001B[39m \u001B[38;5;66;03m# layer_outputs = hidden-states, key-value-states (self-attention position bias), (self-attention weights),\u001B[39;00m\n\u001B[32m    785\u001B[39m \u001B[38;5;66;03m# (cross-attention position bias), (cross-attention weights)\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/llm/lib/python3.12/site-packages/transformers/modeling_layers.py:93\u001B[39m, in \u001B[36mGradientCheckpointingLayer.__call__\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m     90\u001B[39m         logger.warning_once(message)\n\u001B[32m     92\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._gradient_checkpointing_func(partial(\u001B[38;5;28msuper\u001B[39m().\u001B[34m__call__\u001B[39m, **kwargs), *args)\n\u001B[32m---> \u001B[39m\u001B[32m93\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m.\u001B[49m\u001B[34;43m__call__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/llm/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1749\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1750\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1751\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/llm/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1757\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1758\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1759\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1760\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1761\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1762\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1764\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1765\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/llm/lib/python3.12/site-packages/transformers/models/t5/modeling_t5.py:454\u001B[39m, in \u001B[36mT5Block.forward\u001B[39m\u001B[34m(self, hidden_states, attention_mask, position_bias, encoder_hidden_states, encoder_attention_mask, encoder_decoder_position_bias, past_key_values, use_cache, output_attentions, return_dict, cache_position)\u001B[39m\n\u001B[32m    440\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mforward\u001B[39m(\n\u001B[32m    441\u001B[39m     \u001B[38;5;28mself\u001B[39m,\n\u001B[32m    442\u001B[39m     hidden_states,\n\u001B[32m   (...)\u001B[39m\u001B[32m    452\u001B[39m     cache_position=\u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[32m    453\u001B[39m ):\n\u001B[32m--> \u001B[39m\u001B[32m454\u001B[39m     self_attention_outputs = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mlayer\u001B[49m\u001B[43m[\u001B[49m\u001B[32;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    455\u001B[39m \u001B[43m        \u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    456\u001B[39m \u001B[43m        \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m=\u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    457\u001B[39m \u001B[43m        \u001B[49m\u001B[43mposition_bias\u001B[49m\u001B[43m=\u001B[49m\u001B[43mposition_bias\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    458\u001B[39m \u001B[43m        \u001B[49m\u001B[43mpast_key_values\u001B[49m\u001B[43m=\u001B[49m\u001B[43mpast_key_values\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    459\u001B[39m \u001B[43m        \u001B[49m\u001B[43muse_cache\u001B[49m\u001B[43m=\u001B[49m\u001B[43muse_cache\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    460\u001B[39m \u001B[43m        \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[43m=\u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    461\u001B[39m \u001B[43m        \u001B[49m\u001B[43mcache_position\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcache_position\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    462\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    463\u001B[39m     hidden_states = self_attention_outputs[\u001B[32m0\u001B[39m]\n\u001B[32m    464\u001B[39m     attention_outputs = self_attention_outputs[\u001B[32m1\u001B[39m:]  \u001B[38;5;66;03m# Keep self-attention outputs and relative position weights\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/llm/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1749\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1750\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1751\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/llm/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1757\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1758\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1759\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1760\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1761\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1762\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1764\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1765\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/llm/lib/python3.12/site-packages/transformers/models/t5/modeling_t5.py:376\u001B[39m, in \u001B[36mT5LayerSelfAttention.forward\u001B[39m\u001B[34m(self, hidden_states, attention_mask, position_bias, past_key_values, use_cache, output_attentions, cache_position)\u001B[39m\n\u001B[32m    366\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mforward\u001B[39m(\n\u001B[32m    367\u001B[39m     \u001B[38;5;28mself\u001B[39m,\n\u001B[32m    368\u001B[39m     hidden_states,\n\u001B[32m   (...)\u001B[39m\u001B[32m    374\u001B[39m     cache_position=\u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[32m    375\u001B[39m ):\n\u001B[32m--> \u001B[39m\u001B[32m376\u001B[39m     normed_hidden_states = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mlayer_norm\u001B[49m\u001B[43m(\u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    377\u001B[39m     attention_output = \u001B[38;5;28mself\u001B[39m.SelfAttention(\n\u001B[32m    378\u001B[39m         normed_hidden_states,\n\u001B[32m    379\u001B[39m         mask=attention_mask,\n\u001B[32m   (...)\u001B[39m\u001B[32m    384\u001B[39m         cache_position=cache_position,\n\u001B[32m    385\u001B[39m     )\n\u001B[32m    386\u001B[39m     hidden_states = hidden_states + \u001B[38;5;28mself\u001B[39m.dropout(attention_output[\u001B[32m0\u001B[39m])\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/llm/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1749\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1750\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1751\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/llm/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1757\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1758\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1759\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1760\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1761\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1762\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1764\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1765\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/llm/lib/python3.12/site-packages/transformers/models/t5/modeling_t5.py:62\u001B[39m, in \u001B[36mT5LayerNorm.forward\u001B[39m\u001B[34m(self, hidden_states)\u001B[39m\n\u001B[32m     55\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, hidden_states):\n\u001B[32m     56\u001B[39m     \u001B[38;5;66;03m# T5 uses a layer_norm which only scales and doesn't shift, which is also known as Root Mean\u001B[39;00m\n\u001B[32m     57\u001B[39m     \u001B[38;5;66;03m# Square Layer Normalization https://huggingface.co/papers/1910.07467 thus variance is calculated\u001B[39;00m\n\u001B[32m     58\u001B[39m     \u001B[38;5;66;03m# w/o mean and there is no bias. Additionally we want to make sure that the accumulation for\u001B[39;00m\n\u001B[32m     59\u001B[39m     \u001B[38;5;66;03m# half-precision inputs is done in fp32\u001B[39;00m\n\u001B[32m     61\u001B[39m     variance = hidden_states.to(torch.float32).pow(\u001B[32m2\u001B[39m).mean(-\u001B[32m1\u001B[39m, keepdim=\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[32m---> \u001B[39m\u001B[32m62\u001B[39m     hidden_states = hidden_states * \u001B[43mtorch\u001B[49m\u001B[43m.\u001B[49m\u001B[43mrsqrt\u001B[49m\u001B[43m(\u001B[49m\u001B[43mvariance\u001B[49m\u001B[43m \u001B[49m\u001B[43m+\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mvariance_epsilon\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     64\u001B[39m     \u001B[38;5;66;03m# convert into half-precision if necessary\u001B[39;00m\n\u001B[32m     65\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.weight.dtype \u001B[38;5;129;01min\u001B[39;00m [torch.float16, torch.bfloat16]:\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 8
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
